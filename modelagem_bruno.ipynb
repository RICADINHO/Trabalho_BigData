{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8619c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4bea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_limitados=dados.limit(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_limitados = ( dados_limitados\n",
    "            .withColumn(\"Delay\", \n",
    "                                F.when((F.col(\"ArrDelay\") > 15), 1).otherwise(0))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_limitados = dados_limitados.drop('Operated_or_Branded_Code_Share_Partners','DepDelay','ArrDelay'\n",
    "                                       ,'ArrTime' ,'TaxiIn','WheelsOn')\n",
    "dados_limitados.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "\n",
    "# The columns at stake\n",
    "cols_non_numeric = [field.name for field in dados_limitados.schema.fields if isinstance(\n",
    "    field.dataType, T.TimestampType) or isinstance(field.dataType, T.StringType)]\n",
    "cols_numeric = [col for col in dados_limitados.columns if col not in cols_non_numeric]\n",
    "\n",
    "# Recall columns at stake\n",
    "print(f'Non-numeric columns: {cols_non_numeric}')\n",
    "print(f'Numeric columns: {cols_numeric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582364ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which columns not to be used as features. \n",
    "cols_not_features = ['Delay']\n",
    "\n",
    "# Set columns to be used by StringIndexer() and OneHotEncoder()\n",
    "\n",
    "categorical_cols = [i for i in cols_non_numeric if i not in cols_not_features]\n",
    "non_categorical_cols = [i for i in cols_numeric if i not in cols_not_features]\n",
    "index_output_cols = [x + ' Index' for x in categorical_cols]\n",
    "ohe_output_cols = [x + ' OHE' for x in categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling an array with the features to be used by the algorithm,\n",
    "# with the help of StringIndexer(), OneHotEncoder() and vectorAssembler()\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)\n",
    "\n",
    "# Put all input features into a single vector, by using a transformer\n",
    "assembler_inputs = ohe_output_cols + non_categorical_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "print(f'Input features to be used (OHE were categorical):\\n {assembler_inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbfaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "# Two dataframes for training and validation respectively, with a split size of 70/30 (%)\n",
    "df_train, df_validation = dados_limitados.randomSplit([0.7, 0.3], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train.write.mode('overwrite').parquet(\"trans-train_total\")\n",
    "df_validation.write.mode('overwrite').parquet(\"trans-val_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we already got the data split, delete df_clean to free memory space\n",
    "del dados_limitados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b192e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC algorithm\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1, labelCol='Delay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a ML pipeline configuration, holding the sequence of the four stages previously set:\n",
    "# 1. string_indexer\n",
    "# 2. ohe_encoder\n",
    "# 3. vec_assembler (related to assembling features into vector)\n",
    "# 4. lsvc (related to ML estimator)\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer,ohe_encoder,vec_assembler,lsvc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3291c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in the pipeline for further use, should it be required\n",
    "pipeline.save('pipeline-LinearSVM_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf90239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for further use, should it be required.\n",
    "model.save('model-LinearSVM_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aebf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions by applying the verification data to the transformer\n",
    "df_predictions = model.transform(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9dffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns of interest: features, rawPrediction, prediction, Fraud\n",
    "df_predictions_eval = df_predictions.select('features', \n",
    "                    'rawPrediction', 'prediction', 'Delay')\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol='Delay',\n",
    "                                                 rawPredictionCol='rawPrediction',\n",
    "                                                 metricName='areaUnderROC')\n",
    "    \n",
    "area_under_ROC = binary_evaluator.evaluate(df_predictions_eval)\n",
    "\n",
    "# Print out result\n",
    "print(f'Metric areaUnderROC = {area_under_ROC}')\n",
    "#df_predictions_eval.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cfb504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting of the kind of predictions made\n",
    "df_confusion_matrix = df_predictions_eval.groupBy('prediction','Delay').count()\n",
    "df_confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "tp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Delay')==1)).first()\n",
    "tn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Delay')==0)).first()\n",
    "fp = df_confusion_matrix.filter((F.col('prediction')==1.0) & (F.col('Delay')==0)).first()\n",
    "fn = df_confusion_matrix.filter((F.col('prediction')==0.0) & (F.col('Delay')==1)).first()\n",
    "\n",
    "confmat = {'TP': 0.0, 'TN': 0.0, 'FP': 0.0, 'FN': 0.0}\n",
    "if (tp):\n",
    "    confmat['TP'] = tp['count'] * 1.0\n",
    "if (tn):\n",
    "    confmat['TN'] = tn['count'] * 1.0\n",
    "if (fp):\n",
    "    confmat['FP'] = fp['count'] * 1.0\n",
    "if (fn):\n",
    "    confmat['FN'] = fn['count'] * 1.0\n",
    "\n",
    "confmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194190b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the confusion matrix, computed the evaluation matrics:\n",
    "#   accuracy, precision, recall, specifity and F1 score\n",
    "\n",
    "# PS: Check divisons by 0.0\n",
    "accuracy = (confmat['TP'] + confmat['TN']) / (confmat['TP'] + confmat['TN'] + confmat['FP'] + confmat['FN'])\n",
    "precision = confmat['TP'] / (confmat['TP'] + confmat['FP'])\n",
    "recall = confmat['TP'] / (confmat['TP'] + confmat['FN'])\n",
    "specificity = confmat['TN'] / (confmat['TN'] + confmat['FP'])\n",
    "f1score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "print('Evaluation metrics based on the confusion matrix:')\n",
    "print(f' Accuracy = {accuracy}')\n",
    "print(f' Precision = {precision}')\n",
    "print(f' Recall = {recall}')\n",
    "print(f' Specifity = {specificity}')\n",
    "print(f' F1 score = {f1score}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
